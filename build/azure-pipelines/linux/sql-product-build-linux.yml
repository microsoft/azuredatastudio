steps:
  - script: |
      mkdir -p .build
      echo -n $BUILD_SOURCEVERSION > .build/commit
      echo -n $VSCODE_QUALITY > .build/quality
    displayName: Prepare cache flag

  - task: 1ESLighthouseEng.PipelineArtifactCaching.RestoreCacheV1.RestoreCache@1
    inputs:
      keyfile: 'build/.cachesalt, .build/commit, .build/quality'
      targetfolder: '.build, out-build, out-vscode-min, out-vscode-reh-min, out-vscode-reh-web-min'
      vstsFeed: 'BuildCache'
      platformIndependent: true
      alias: 'Compilation'

  - script: |
      set -e
      exit 1
    displayName: Check RestoreCache
    condition: and(succeeded(), ne(variables['CacheRestored-Compilation'], 'true'))

  - task: NodeTool@0
    inputs:
      versionSpec: '10.15.1'

  - task: geeklearningio.gl-vsts-tasks-yarn.yarn-installer-task.YarnInstaller@3
    inputs:
      versionSpec: "1.x"

  - task: AzureKeyVault@1
    displayName: 'Azure Key Vault: Get Secrets'
    inputs:
      azureSubscription: 'ClientToolsInfra_670062 (88d5392f-a34f-4769-b405-f597fc533613)'
      KeyVaultName: ado-secrets

  - script: |
      set -e
      cat << EOF > ~/.netrc
      machine github.com
      login azuredatastudio
      password $(github-distro-mixin-password)
      EOF

      git config user.email "andresse@microsoft.com"
      git config user.name "AzureDataStudio"
    displayName: Prepare tooling

  - script: |
      set -e
      git remote add distro "https://github.com/$(VSCODE_MIXIN_REPO).git"
      git fetch distro
      git merge $(node -p "require('./package.json').distro")
    displayName: Merge distro

  - task: 1ESLighthouseEng.PipelineArtifactCaching.RestoreCacheV1.RestoreCache@1
    inputs:
      keyfile: 'build/.cachesalt, .yarnrc, remote/.yarnrc, **/yarn.lock, !**/node_modules/**/yarn.lock, !**/.*/**/yarn.lock, !samples/**/yarn.lock'
      targetfolder: '**/node_modules, !**/node_modules/**/node_modules, !samples/**/node_modules'
      vstsFeed: 'BuildCache'

  - script: |
      set -e
      CHILD_CONCURRENCY=1 yarn --frozen-lockfile
    displayName: Install dependencies
    condition: and(succeeded(), ne(variables['CacheRestored'], 'true'))

  - task: 1ESLighthouseEng.PipelineArtifactCaching.SaveCacheV1.SaveCache@1
    inputs:
      keyfile: 'build/.cachesalt, .yarnrc, remote/.yarnrc, **/yarn.lock, !**/node_modules/**/yarn.lock, !**/.*/**/yarn.lock, !samples/**/yarn.lock'
      targetfolder: '**/node_modules, !**/node_modules/**/node_modules, !samples/**/node_modules'
      vstsFeed: 'BuildCache'
    condition: and(succeeded(), ne(variables['CacheRestored'], 'true'))

  - script: |
      set -e
      yarn postinstall
    displayName: Run postinstall scripts
    condition: and(succeeded(), eq(variables['CacheRestored'], 'true'))

  - script: |
      set -e
      node build/azure-pipelines/mixin
    displayName: Mix in quality

  - script: |
      set -e
      yarn gulp install-sqltoolsservice
      yarn gulp install-ssmsmin
    displayName: Install extension binaries

  - script: |
      set -e
      yarn gulp vscode-linux-x64-min-ci
      yarn gulp vscode-reh-linux-x64-min-ci
      yarn gulp vscode-reh-web-linux-x64-min-ci
    displayName: Build

  - script: |
      set -e
      service xvfb start
    displayName: Start xvfb
    condition: and(succeeded(), eq(variables['VSCODE_STEP_ON_IT'], 'false'))

  # - script: |
  #     set -e
  #     docker build -t ads-server-web -f build/azure-pipelines/docker/Dockerfile $(Build.SourcesDirectory)/../vscode-reh-linux-x64
  #     docker save ads-server-web | gzip > server-docker-web.tar.gz
  #     cp server-docker-web.tar.gz $(Build.ArtifactStagingDirectory)
  #   displayName: "Create docker image"

  - script: |
      set -e
      yarn gulp compile-extensions
      yarn gulp package-external-extensions
    displayName: Package External extensions

  - task: ArchiveFiles@2 # WHY ARE WE DOING THIS?
    displayName: 'Archive build scripts source'
    inputs:
      rootFolderOrFile: '$(Build.SourcesDirectory)/build'
      archiveType: tar
      archiveFile: '$(Build.BinariesDirectory)/source.tar.gz'

  - task: PublishBuildArtifacts@1 # WHY ARE WE DOING THIS?
    displayName: 'Publish Artifact: build scripts source'
    inputs:
      PathtoPublish: '$(Build.BinariesDirectory)/source.tar.gz'
      ArtifactName: source

  - script: |
      APP_ROOT=$(agent.builddirectory)/azuredatastudio-linux-x64
      APP_NAME=$(node -p "require(\"$APP_ROOT/resources/app/product.json\").applicationName")
      INTEGRATION_TEST_ELECTRON_PATH="$APP_ROOT/$APP_NAME" \
      DISPLAY=:10 ./scripts/test-extensions-unit.sh
    displayName: 'Run Stable Extension Unit Tests'
    condition: and(succeeded(), eq(variables['RUN_TESTS'], 'true'))

  - script: |
      APP_ROOT=$(agent.builddirectory)/azuredatastudio-linux-x64
      APP_NAME=$(node -p "require(\"$APP_ROOT/resources/app/product.json\").applicationName")
      INTEGRATION_TEST_ELECTRON_PATH="$APP_ROOT/$APP_NAME" \
      DISPLAY=:10 ./scripts/test-extensions-unit-unstable.sh
    displayName: 'Run Unstable Extension Unit Tests'
    continueOnError: true
    condition: and(succeeded(), eq(variables['RUN_UNSTABLE_TESTS'], 'true'))

  - script: |
      set -e
      yarn gulp vscode-linux-x64-build-deb
    displayName: Build Deb

  - script: |
      set -e
      yarn gulp vscode-linux-x64-build-rpm
    displayName: Build Rpm

  - task: ArchiveFiles@1 # WHY ARE WE DOING THIS?
    displayName: 'Archive files '
    inputs:
      rootFolder: '$(Build.SourcesDirectory)/../azuredatastudio-linux-x64'
      archiveType: tar
      archiveFile: '$(Build.ArtifactStagingDirectory)/azuredatastudio-linux-x64.tar.gz'

  - task: CopyFiles@2
    displayName: 'Copy Files to: $(Build.ArtifactStagingDirectory) (deb)'
    inputs:
      SourceFolder: '$(Build.SourcesDirectory)/.build/linux/deb/amd64/deb'
      Contents: '*.deb'
      TargetFolder: '$(Build.ArtifactStagingDirectory)'

  - task: CopyFiles@2
    displayName: 'Copy Files to: $(Build.ArtifactStagingDirectory) (rpm)'
    inputs:
      SourceFolder: '$(Build.SourcesDirectory)/.build/linux/rpm/x86_64/'
      Contents: '*.rpm'
      TargetFolder: '$(Build.ArtifactStagingDirectory)'

  - task: CopyFiles@2
    displayName: 'Copy Files to: $(Build.ArtifactStagingDirectory)/vsix'
    inputs:
      SourceFolder: '$(Build.SourcesDirectory)/../vsix'
      TargetFolder: '$(Build.ArtifactStagingDirectory)/vsix'

  - script: |
      set -e
      cd $(agent.builddirectory)
      tar --owner=0 --group=0 -czf azuredatastudio-server-linux-x64.tar.gz azuredatastudio-reh-linux-x64
      cp azuredatastudio-server-linux-x64.tar.gz $(Build.ArtifactStagingDirectory)
    displayName: 'Package server'

  - script: | # WHY ARE WE DOING THIS?
      set -e
      PACKAGEJSON="$(Build.SourcesDirectory)/package.json"
      VERSION=$(node -p "require(\"$PACKAGEJSON\").version")
      COMMIT_ID=$(git rev-parse HEAD)

      echo -e "{  \"version\": \"$VERSION\", \"quality\": \"$VSCODE_QUALITY\", \"commit\": \"$COMMIT_ID\" }" > "$(Build.ArtifactStagingDirectory)/version.json"
    displayName: 'Create version.json'

  - script: | # WHY ARE WE DOING THIS?
      set -e
      for f in *
      do
      shasum -a 256 "$f" >> sha256hashes.txt
      done
    workingDirectory: '$(Build.ArtifactStagingDirectory)'
    displayName: 'Get SHA256 Hashes'
    continueOnError: true

  - task: PublishBuildArtifacts@1
    displayName: 'Publish Artifact: drop'

  - task: PublishTestResults@2
    displayName: 'Publish Test Results test-results.xml'
    inputs:
      testResultsFiles: 'test-results.xml'
      searchFolder: '$(Build.SourcesDirectory)'
    continueOnError: true
    condition: and(succeeded(), eq(variables['RUN_TESTS'], 'true'))

  - task: PublishCodeCoverageResults@1
    displayName: 'Publish code coverage from $(Build.SourcesDirectory)/.build/coverage/cobertura-coverage.xml'
    inputs:
      codeCoverageTool: Cobertura
      summaryFileLocation: '$(Build.SourcesDirectory)/.build/coverage/cobertura-coverage.xml'
      reportDirectory: '$(Build.SourcesDirectory)/.build/coverage'
    continueOnError: true

  - task: ms.vss-governance-buildtask.governance-build-task-component-detection.ComponentGovernanceComponentDetection@0
    displayName: 'Component Detection'
    inputs:
      failOnAlert: true

  - script: 'echo "##vso[build.addbuildtag]Scheduled" '
    displayName: 'Add scheduled tag if needed'
    condition: and(in(variables['Agent.JobStatus'], 'Succeeded'), eq(variables['Build.Reason'], 'Schedule'))

  - script: 'echo "##vso[build.addbuildtag]PerfTestCandidate" '
    displayName: 'Add PerfTestCandidate tag if needed'
    condition: and(in(variables['Agent.JobStatus'], 'Succeeded'), eq(variables['VSCODE_QUALITY'], 'insider'))
