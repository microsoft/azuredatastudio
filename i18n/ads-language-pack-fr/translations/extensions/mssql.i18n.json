{
	"": [
		"--------------------------------------------------------------------------------------------",
		"Copyright (c) Microsoft Corporation. All rights reserved.",
		"Licensed under the Source EULA. See License.txt in the project root for license information.",
		"--------------------------------------------------------------------------------------------",
		"Do not edit this file. It is machine generated."
	],
	"version": "1.0.0",
	"contents": {
		"dist/dashboard/serviceEndpoints": {
			"copyText": "Copier",
			"endpoint.appproxy": "Proxy d'application",
			"endpoint.controller": "Service de gestion de cluster",
			"endpoint.gateway": "Passerelle d'accès aux fichiers HDFS, à Spark",
			"endpoint.grafana": "Tableau de bord des métriques",
			"endpoint.kibana": "Tableau de bord de recherche dans les journaux",
			"endpoint.livy": "Proxy pour exécuter les instructions, travaux, applications Spark",
			"endpoint.managementproxy": "Proxy de gestion",
			"endpoint.mgmtproxy": "Proxy de gestion",
			"endpoint.sparkHistory": "Tableau de bord de gestion et de supervision de travaux Spark",
			"endpoint.sqlServerEndpoint": "Front-end de l'instance maître SQL Server",
			"endpoint.webhdfs": "Proxy du système de fichiers HDFS",
			"endpoint.yarnHistory": "Tableau de bord de diagnostic et de supervision Spark",
			"grafana": "Tableau de bord des métriques",
			"kibana": "Tableau de bord de recherche dans les journaux",
			"sparkHistory": "Tableau de bord de gestion et de supervision de travaux Spark",
			"yarnHistory": "Tableau de bord de diagnostic et de supervision Spark"
		},
		"dist/features": {
			"mssql.canceledLinkedAzureAccountSelection": "Azure Data Studio doit contacter Azure Key Vault pour accéder à une clé principale de colonne pour Always Encrypted, mais aucun compte Azure lié n'a été sélectionné. Réessayez la requête et sélectionnez un compte Azure lié quand vous y êtes invité.",
			"mssql.chooseLinkedAzureAccount": "Sélectionnez un compte Azure lié :",
			"mssql.insufficientlyPrivelagedAzureAccount": "Le compte Azure configuré pour {0} n'a pas les autorisations suffisantes pour permettre à Azure Key Vault d'accéder à une clé principale de colonne pour Always Encrypted.",
			"mssql.missingLinkedAzureAccount": "Azure Data Studio doit contacter Azure Key Vault pour accéder à une clé principale de colonne pour Always Encrypted, mais aucun compte Azure lié n'est disponible. Ajoutez un compte Azure lié et réessayez la requête."
		},
		"dist/hdfs/hdfsModel": {
			"mssql.recursivePermissionOpError": "Erreur d'application des changements d'autorisation : {0}",
			"mssql.recursivePermissionOpProgress": "Application des changements d'autorisation à '{0}'.",
			"mssql.recursivePermissionOpStarted": "Application des changements d'autorisation de manière récursive sous '{0}'",
			"mssql.recursivePermissionOpSucceeded": "Les changements d'autorisation ont été appliqués."
		},
		"dist/hdfs/webhdfs": {
			"webhdfs.httpError400": "Demande incorrecte",
			"webhdfs.httpError401": "Non autorisé",
			"webhdfs.httpError403": "Interdit",
			"webhdfs.httpError404": "Introuvable",
			"webhdfs.httpError500": "Erreur de serveur interne",
			"webhdfs.invalidDataStructure": "Structure de données non valide",
			"webhdfs.missingProperties": "Impossible de créer le client WebHDFS en raison d'options manquantes : ${0}",
			"webhdfs.undefinedArgument": "'${0}' n'est pas défini.",
			"webhdfs.unexpectedRedirect": "Redirection inattendue",
			"webhdfs.unknownError": "Erreur inconnue"
		},
		"dist/localizedConstants": {
			"msgMissingNodeContext": "Commande de nœud appelée sans aucun nœud",
			"mssql.accessHeader": "Accès",
			"mssql.addLabel": "Ajouter",
			"mssql.addUserOrGroup": "Ajouter un utilisateur ou un groupe",
			"mssql.apply": "Appliquer",
			"mssql.applyRecursively": "Appliquer de façon récursive",
			"mssql.defaultHeader": "Par défaut",
			"mssql.defaultUserAndGroups": "Utilisateur et groupes par défaut",
			"mssql.delete": "Supprimer",
			"mssql.enterNamePlaceholder": "Entrer un nom",
			"mssql.errorApplyingAclChanges": "Une erreur inattendue s'est produite pendant l'application des changements : {0}",
			"mssql.everyone": "Tous les autres",
			"mssql.executeHeader": "Exécuter",
			"mssql.group": "Groupe",
			"mssql.groupLabel": "Groupe",
			"mssql.inheritDefaultsLabel": "Hériter les valeurs par défaut",
			"mssql.locationTitle": "Emplacement : ",
			"mssql.manageAccessTitle": "Gérer l'accès",
			"mssql.namedUsersAndGroups": "Utilisateurs et groupes nommés",
			"mssql.owner": "Propriétaire",
			"mssql.ownerPostfix": " - Propriétaire",
			"mssql.owningGroupPostfix": " - Groupe propriétaire",
			"mssql.permissionsTitle": "Autorisations",
			"mssql.readHeader": "Lire",
			"mssql.stickyHeader": "Sticky Bit",
			"mssql.userLabel": "Utilisateur",
			"mssql.userOrGroupIcon": "Icône Utilisateur ou Groupe",
			"mssql.writeHeader": "Écrire",
			"sparkConnectionRequired": "Connectez-vous au cluster Spark pour pouvoir voir l'historique de {0}.",
			"sparkJobSubmission.GetApplicationIdFailed": "L'obtention de l'ID d'application a échoué. {0}",
			"sparkJobSubmission.LocalFileDestinationHint": "Le fichier local est chargé dans HDFS. ",
			"sparkJobSubmission.LocalFileNotExisted": "Le fichier local {0} n'existe pas. ",
			"sparkJobSubmission.NoSqlBigDataClusterFound": "Aucun cluster Big Data SQL Server.",
			"sparkJobSubmission.PrepareSubmitJob": "Envoi du travail {0}... ",
			"sparkJobSubmission.PrepareUploadingFile": "Chargement du fichier du {0} local vers le dossier HDFS : {1}",
			"sparkJobSubmission.SparkHistoryLinkMessage": "URL de l'historique Spark : {0} ",
			"sparkJobSubmission.SubmissionEndMessage": ".......................... Fin d'envoi du travail Spark ............................",
			"sparkJobSubmission.SubmitJobFailed": "L'envoi du travail Spark a échoué. {0} ",
			"sparkJobSubmission.SubmitJobFinished": "Le travail Spark a été envoyé.",
			"sparkJobSubmission.UploadingFileFailed": "Le chargement du fichier dans le cluster a échoué. {0}",
			"sparkJobSubmission.UploadingFileSucceeded": "Le fichier a été chargé dans le cluster.",
			"sparkJobSubmission.YarnUIMessage": "URL de YarnUI : {0} "
		},
		"dist/main": {
			"msgSampleCodeDataFrame": "Cet exemple de code charge le fichier dans un cadre de données et affiche les 10 premiers résultats.",
			"mssql.errorConvertingToNotebook": "Erreur pendant la conversion du document SQL au format Notebook. Erreur : {0}",
			"mssql.errorConvertingToSQL": "Erreur pendant la conversion du document Notebook au format SQL. Erreur : {0}",
			"noController": "Le point de terminaison de contrôleur de cette instance est introuvable",
			"notebookFileType": "Notebooks",
			"unsupportedFileType": "Seuls les notebooks .ipynb sont pris en charge"
		},
		"dist/objectExplorerNodeProvider/cancelableStream": {
			"streamCanceled": "Opération de flux annulée par l'utilisateur"
		},
		"dist/objectExplorerNodeProvider/command": {
			"cancel": "Annuler l'opération ?",
			"cancelTooltip": "Annuler",
			"mssql.searchServers": "Rechercher dans les noms de serveur",
			"progress": "$(sync~spin) {0}..."
		},
		"dist/objectExplorerNodeProvider/connection": {
			"connectionInfoOptionsMissingProperties": "Des propriétés sont manquantes dans connectionInfo.options : {0}",
			"connectionInfoOptionsUndefined": "ConnectionInfo.options n'est pas défini.",
			"connectionInfoUndefined": "ConnectionInfo n'est pas défini."
		},
		"dist/objectExplorerNodeProvider/fileSources": {
			"maxSizeNotice": "REMARQUE : Ce fichier a été tronqué au niveau de {0} pour l'aperçu. ",
			"maxSizeReached": "Le fichier a été tronqué au niveau de {0} pour l'aperçu."
		},
		"dist/objectExplorerNodeProvider/hdfsCommands": {
			"allFiles": "Tous les fichiers",
			"copyPathError": "Erreur de copie du chemin : {0}",
			"deleteError": "Erreur de suppression des fichiers : {0}",
			"enterDirName": "Entrer le nom du répertoire",
			"lblUploadFiles": "Charger",
			"makingDir": "Création du répertoire",
			"manageAccessError": "Une erreur inattendue s'est produite à l'ouverture de la boîte de dialogue Gérer l'accès : {0}",
			"mkDirError": "Erreur de création du répertoire : {0}",
			"mkdirCanceled": "L'opération a été annulée",
			"msgDeleteFile": "Voulez-vous vraiment supprimer ce fichier ?",
			"msgDeleteFolder": "Voulez-vous vraiment supprimer ce dossier et son contenu ?",
			"previewError": "Erreur d'affichage de l'aperçu du fichier : {0}",
			"previewing": "Génération de l'aperçu",
			"saveCanceled": "L'opération d'enregistrement a été annulée",
			"saveError": "Erreur d'enregistrement du fichier : {0}",
			"saving": "Enregistrement des fichiers HDFS",
			"uploadCanceled": "L'opération de chargement a été annulée",
			"uploadError": "Erreur de chargement des fichiers : {0}",
			"uploading": "Chargement des fichiers dans HDFS"
		},
		"dist/objectExplorerNodeProvider/hdfsProvider": {
			"errDeleteConnectionNode": "Impossible de supprimer une connexion. Seuls les sous-dossiers et les fichiers peuvent être supprimés.",
			"errorExpanding": "Erreur : {0}"
		},
		"dist/objectExplorerNodeProvider/objectExplorerNodeProvider": {
			"hdfsFolder": "HDFS",
			"notifyError": "Erreur de notification du changement de nœud : {0}",
			"prmptPwd": "Fournissez le mot de passe de connexion à HDFS :",
			"promptUsername": "Fournissez le nom d'utilisateur pour se connecter à HDFS :",
			"rootLabel": "Racine",
			"sessionNotFound": "La session du nœud {0} n'existe pas"
		},
		"dist/prompts/confirm": {
			"msgNo": "Non",
			"msgYes": "Oui"
		},
		"dist/sparkFeature/dialog/dialogCommands": {
			"errorNotSqlBigDataCluster": "Le serveur sélectionné n'appartient pas à un cluster Big Data SQL Server",
			"selectOtherServer": "Sélectionner un autre serveur SQL",
			"sparkJobSubmission.GetFilePathFromSelectedNodeFailed": "Erreur d'obtention du chemin de fichier : {0}",
			"sparkJobSubmission.NoSqlSelected": "Aucun serveur SQL sélectionné.",
			"sparkJobSubmission.PleaseSelectSqlWithCluster": "Sélectionnez SQL Server avec le cluster Big Data."
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkAdvancedTab": {
			"sparkJobSubmission.AdvancedTabName": "AVANCÉ",
			"sparkJobSubmission.ReferenceFilesList": "Fichiers de référence",
			"sparkJobSubmission.ReferenceFilesListTooltip": "Fichiers à placer dans le répertoire de travail de l'exécuteur. Le chemin de fichier doit être un chemin HDFS. Plusieurs chemins doivent être séparés par un point-virgule (;)",
			"sparkJobSubmission.ReferenceJarList": "Fichiers JAR de référence",
			"sparkJobSubmission.ReferenceJarListToolTip": "Fichiers JAR à placer dans le répertoire de travail de l'exécuteur. Le chemin de fichier JAR doit être un chemin HDFS. Plusieurs chemins doivent être séparés par un point-virgule (;)",
			"sparkJobSubmission.ReferencePyList": "Fichiers py de référence",
			"sparkJobSubmission.ReferencePyListTooltip": "Fichiers py à placer dans le répertoire de travail de l'exécuteur. Le chemin de fichier doit être un chemin HDFS. Plusieurs chemins doivent être séparés par un point-virgule (;)",
			"sparkJobSubmission.configValues": "Valeurs de configuration",
			"sparkJobSubmission.configValuesTooltip": "Liste des paires nom/valeur contenant des valeurs de configuration Spark. Encodé sous forme de dictionnaire JSON. Exemple : « {\"name\":\"value\", \"name2\":\"value2\"} ».",
			"sparkJobSubmission.driverCores": "Cœurs de pilote",
			"sparkJobSubmission.driverCoresTooltip": "Quantité de cœurs de processeur à allouer au pilote.",
			"sparkJobSubmission.driverMemory": "Mémoire de pilote",
			"sparkJobSubmission.driverMemoryTooltip": "Quantité de mémoire à allouer au pilote. Spécifiez des unités dans la valeur. Par exemple, 512M ou 2G.",
			"sparkJobSubmission.executorCores": "Cœurs d'exécuteur",
			"sparkJobSubmission.executorCoresTooltip": "Quantité de cœurs de processeur à allouer à l'exécuteur.",
			"sparkJobSubmission.executorCount": "Nombre d'exécuteurs",
			"sparkJobSubmission.executorCountTooltip": "Nombre d'instances de l'exécuteur à exécuter.",
			"sparkJobSubmission.executorMemory": "Mémoire d'exécuteur",
			"sparkJobSubmission.executorMemoryTooltip": "Quantité de mémoire à allouer à l'exécuteur. Spécifiez des unités dans la valeur. Par exemple, 512M ou 2G.",
			"sparkJobSubmission.queueName": "Nom de la file d'attente",
			"sparkJobSubmission.queueNameTooltip": "Nom de la file d'attente Spark où exécuter la session."
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkConfigurationTab": {
			"sparkJobSubmission.Arguments": "Arguments",
			"sparkJobSubmission.ArgumentsTooltip": "Arguments de ligne de commande utilisés dans votre classe principale, plusieurs arguments doivent être séparés par un espace.",
			"sparkJobSubmission.FilePathPlaceHolder": "Chemin d'un fichier .jar ou .py",
			"sparkJobSubmission.GeneralTabName": "GÉNÉRAL",
			"sparkJobSubmission.HDFSFileNotExisted": "Le fichier HDFS spécifié n'existe pas. ",
			"sparkJobSubmission.HDFSFileNotExistedWithPath": "{0} n'existe pas dans le cluster ou une exception est levée. ",
			"sparkJobSubmission.JobName": "Nom du travail",
			"sparkJobSubmission.JobNamePlaceHolder": "Entrer un nom...",
			"sparkJobSubmission.LocalFileDestinationHintWithPath": "Le fichier local sélectionné est chargé dans HDFS : {0}",
			"sparkJobSubmission.MainClass": "Classe principale",
			"sparkJobSubmission.MainFilePath": "Fichier JAR/py",
			"sparkJobSubmission.NotSpecifyJARPYPath": "Le fichier JAR/py de propriétés n'est pas spécifié.",
			"sparkJobSubmission.NotSpecifyJobName": "Le nom de travail de la propriété n'est pas spécifié.",
			"sparkJobSubmission.NotSpecifyMainClass": "La classe principale de la propriété n'est pas spécifiée.",
			"sparkJobSubmission.SelectFileError": "Erreur de localisation du fichier en raison de l'erreur : {0}",
			"sparkJobSubmission.SparkCluster": "Cluster Spark",
			"sparkSelectLocalFile": "Sélectionner"
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionDialog": {
			"sparkJobSubmission.DialogCancelButton": "Annuler",
			"sparkJobSubmission.DialogSubmitButton": "Envoyer",
			"sparkJobSubmission.DialogTitleNewJob": "Nouveau travail",
			"sparkJobSubmission.SparkJobSubmissionDialogInitializeError": "Les paramètres de SparkJobSubmissionDialog ne sont pas autorisés",
			"sparkJobSubmission.SubmissionStartMessage": ".......................... Début de l'envoi du travail Spark ..........................",
			"sparkJobSubmission.SubmitSparkJob": "Envoi du travail Spark {0} :"
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionModel": {
			"sparkJobSubmission.GetApplicationIdTimeOut": "Le délai d'obtention de l'ID d'application a expiré. {0}[Journal]   {1}",
			"sparkJobSubmission.LivyBatchIdIsInvalid": "livyBatchId n'est pas valide. ",
			"sparkJobSubmission.PathNotSpecified.": "Le chemin de propriété n'est pas spécifié. ",
			"sparkJobSubmission.SparkJobSubmissionModelInitializeError": "Les paramètres de SparkJobSubmissionModel ne sont pas autorisés",
			"sparkJobSubmission.localFileOrFolderNotSpecified.": "La propriété localeFilePath ou hdfsFolderPath n'est pas spécifiée. ",
			"sparkJobSubmission.submissionArgsIsInvalid": "soumissionArgs n'est pas valide. "
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionService": {
			"sparkJobSubmission.LivyNoBatchIdReturned": "Aucun ID de lot de travail Spark dans la réponse.{0}[Erreur] {1}",
			"sparkJobSubmission.LivyNoLogReturned": "Aucun journal dans la réponse.{0}[Erreur] {1}"
		},
		"dist/sqlClusterLookUp": {
			"bdcConnectError": "Erreur : {0}. ",
			"promptBDCPassword": "Fournissez le mot de passe pour se connecter au contrôleur BDC",
			"promptBDCUsername": "{0}Fournissez le nom d'utilisateur pour se connecter au contrôleur BDC :",
			"usernameAndPasswordRequired": "Le nom d'utilisateur et le mot de passe sont obligatoires"
		},
		"dist/sqlToolsServer": {
			"downloadServiceDoneChannelMsg": "Installation de {0} effectuée",
			"downloadingServiceChannelMsg": "Téléchargement de {0}",
			"downloadingServiceSizeChannelMsg": "({0} Ko)",
			"downloadingServiceStatusMsg": "Téléchargement de {0}",
			"entryExtractedChannelMsg": "{0} extrait ({1}/{2})",
			"failedToStartServiceErrorMsg": "Échec du démarrage de {0}",
			"installedServiceChannelMsg": "{0} installé",
			"installingServiceChannelMsg": "Installation de {0} sur {1}",
			"installingServiceStatusMsg": "Installation de {0}",
			"serviceStartedStatusMsg": "{0} démarré",
			"startingServiceStatusMsg": "Démarrage de {0}"
		},
		"dist/telemetry": {
			"serviceCrashMessage": "Le composant {0} a quitté de manière inattendue. Redémarrez Azure Data Studio.",
			"viewKnownIssuesText": "Voir les problèmes connus"
		},
		"package": {
			"cloud.databaseProperties.azureEdition": "Édition",
			"cloud.databaseProperties.compatibilityLevel": "Niveau de compatibilité",
			"cloud.databaseProperties.owner": "Propriétaire",
			"cloud.databaseProperties.serviceLevelObjective": "Niveau tarifaire",
			"cloud.serverProperties.serverEdition": "Type",
			"cloud.serverProperties.serverVersion": "Version",
			"databasesListProperties.lastBackup": "Dernière sauvegarde",
			"databasesListProperties.name": "Nom",
			"databasesListProperties.size": "Taille (Mo)",
			"databasesListProperties.status": "État",
			"json.format.enable.desc": "Activer/désactiver le formateur JSON par défaut (nécessite un redémarrage)",
			"json.schemas.desc": "Associer des schémas aux fichiers JSON dans le projet actuel",
			"json.schemas.fileMatch.desc": "Tableau de modèles de fichier à mapper durant la résolution de fichiers JSON en schémas.",
			"json.schemas.fileMatch.item.desc": "Modèle de fichier pouvant contenir '*' à mapper durant la résolution de fichiers JSON en schémas.",
			"json.schemas.schema.desc": "Définition de schéma pour l'URL indiquée. Le schéma doit être fourni uniquement pour éviter les accès à l'URL du schéma.",
			"json.schemas.url.desc": "URL de schéma ou chemin relatif d'un schéma dans le répertoire actuel",
			"mssql.configuration.title": "Configuration MSSQL",
			"mssql.connectionOptions.applicationIntent.description": "Déclare le type de charge de travail de l'application pendant la connexion à un serveur",
			"mssql.connectionOptions.applicationIntent.displayName": "Intention d'application",
			"mssql.connectionOptions.applicationName.description": "Nom de l'application",
			"mssql.connectionOptions.applicationName.displayName": "Nom de l'application",
			"mssql.connectionOptions.asynchronousProcessing.description": "Quand la valeur est true, permet d'utiliser la fonctionnalité asynchrone dans le fournisseur de données .Net Framework",
			"mssql.connectionOptions.asynchronousProcessing.displayName": "Traitement asynchrone",
			"mssql.connectionOptions.attachDbFilename.displayName": "Attacher le nom de fichier de base de données",
			"mssql.connectionOptions.attachedDBFileName.description": "Nom de fichier principal, y compris le nom de chemin complet, d'une base de données pouvant être attachée",
			"mssql.connectionOptions.attachedDBFileName.displayName": "Nom du fichier de base de données attaché",
			"mssql.connectionOptions.authType.categoryValues.azureMFA": "Azure Active Directory - Authentification universelle avec prise en charge de MFA",
			"mssql.connectionOptions.authType.categoryValues.integrated": "Authentification Windows",
			"mssql.connectionOptions.authType.categoryValues.sqlLogin": "Connexion SQL",
			"mssql.connectionOptions.authType.description": "Spécifie la méthode d'authentification avec SQL Server",
			"mssql.connectionOptions.authType.displayName": "Type d'authentification",
			"mssql.connectionOptions.columnEncryptionSetting.description": "Active ou désactive Always Encrypted pour la connexion",
			"mssql.connectionOptions.columnEncryptionSetting.displayName": "Always Encrypted",
			"mssql.connectionOptions.connectRetryCount.description": "Nombre de tentatives de restauration de connexion",
			"mssql.connectionOptions.connectRetryCount.displayName": "Nombre de tentatives de connexion",
			"mssql.connectionOptions.connectRetryInterval.description": "Délai entre les tentatives de restauration de connexion",
			"mssql.connectionOptions.connectRetryInterval.displayName": "Intervalle entre les tentatives de connexion",
			"mssql.connectionOptions.connectTimeout.description": "Durée d'attente (en secondes) d'une connexion au serveur avant de terminer la tentative et de générer une erreur",
			"mssql.connectionOptions.connectTimeout.displayName": "Délai d'expiration de la connexion",
			"mssql.connectionOptions.connectionName.description": "Nom personnalisé de la connexion",
			"mssql.connectionOptions.connectionName.displayName": "Nom (facultatif)",
			"mssql.connectionOptions.contextConnection.description": "Quand la valeur est true, indique que la connexion doit provenir du contexte du serveur SQL. Disponible uniquement en cas d'exécution dans le processus SQL Server",
			"mssql.connectionOptions.contextConnection.displayName": "Connexion contextuelle",
			"mssql.connectionOptions.currentLanguage.description": "Nom d'enregistrement de la langue de SQL Server",
			"mssql.connectionOptions.currentLanguage.displayName": "Langage actuel",
			"mssql.connectionOptions.databaseName.description": "Nom du catalogue ou de la base de données de départ dans la source de données",
			"mssql.connectionOptions.databaseName.displayName": "Base de données",
			"mssql.connectionOptions.enclaveAttestationProtocol.categoryValues.AAS": "Azure Attestation",
			"mssql.connectionOptions.enclaveAttestationProtocol.categoryValues.HGS": "Service Guardian hôte",
			"mssql.connectionOptions.enclaveAttestationProtocol.description": "Spécifie un protocole pour attester une enclave côté serveur utilisée avec Always Encrypted avec des enclaves sécurisées",
			"mssql.connectionOptions.enclaveAttestationProtocol.displayName": "Protocole d'attestation",
			"mssql.connectionOptions.enclaveAttestationUrl.description": "Spécifie un point de terminaison pour attester une enclave côté serveur utilisée avec Always Encrypted avec des enclaves sécurisées",
			"mssql.connectionOptions.enclaveAttestationUrl.displayName": "URL d'attestation d'enclave",
			"mssql.connectionOptions.encrypt.description": "Quand la valeur est true, SQL Server utilise le chiffrement SSL pour toutes les données envoyées entre le client et le serveur si le serveur a un certificat installé",
			"mssql.connectionOptions.encrypt.displayName": "Chiffrer",
			"mssql.connectionOptions.failoverPartner.description": "Nom ou adresse réseau de l'instance de SQL Server servant de partenaire de basculement",
			"mssql.connectionOptions.failoverPartner.displayName": "Partenaire de basculement",
			"mssql.connectionOptions.groupName.advanced": "Avancé",
			"mssql.connectionOptions.groupName.connectionResiliency": "Résilience de la connexion",
			"mssql.connectionOptions.groupName.context": "Contexte",
			"mssql.connectionOptions.groupName.initialization": "Initialisation",
			"mssql.connectionOptions.groupName.pooling": "Regroupement",
			"mssql.connectionOptions.groupName.replication": "Réplication",
			"mssql.connectionOptions.groupName.security": "Sécurité",
			"mssql.connectionOptions.groupName.source": "Source",
			"mssql.connectionOptions.loadBalanceTimeout.description": "Durée de vie minimale (en secondes) de cette connexion dans le pool avant d'être détruite",
			"mssql.connectionOptions.loadBalanceTimeout.displayName": "Délai d'expiration de l'équilibrage de charge",
			"mssql.connectionOptions.maxPoolSize.description": "Nombre maximal de connexions autorisées dans le pool",
			"mssql.connectionOptions.maxPoolSize.displayName": "Taille maximale du pool",
			"mssql.connectionOptions.minPoolSize.description": "Nombre minimal de connexions autorisées dans le pool",
			"mssql.connectionOptions.minPoolSize.displayName": "Taille minimale du pool",
			"mssql.connectionOptions.multiSubnetFailover.displayName": "Basculement de plusieurs sous-réseaux",
			"mssql.connectionOptions.multipleActiveResultSets.description": "Quand la valeur est true, plusieurs jeux de résultats peuvent être retournés et lus sur une même connexion",
			"mssql.connectionOptions.multipleActiveResultSets.displayName": "MARS (Multiple Active Result Set)",
			"mssql.connectionOptions.packetSize.description": "Taille en octets des paquets réseau utilisés pour communiquer avec une instance de SQL Server",
			"mssql.connectionOptions.packetSize.displayName": "Taille de paquet",
			"mssql.connectionOptions.password.description": "Indique le mot de passe à utiliser pour la connexion à la source de données",
			"mssql.connectionOptions.password.displayName": "Mot de passe",
			"mssql.connectionOptions.persistSecurityInfo.description": "Quand la valeur est false, les informations de sécurité, comme le mot de passe, ne sont pas retournées dans le cadre de la connexion",
			"mssql.connectionOptions.persistSecurityInfo.displayName": "Conserver les informations de sécurité",
			"mssql.connectionOptions.pooling.description": "Quand la valeur est true, l'objet de connexion est tiré du pool approprié ou, si nécessaire, est créé et ajouté au pool approprié",
			"mssql.connectionOptions.pooling.displayName": "Regroupement",
			"mssql.connectionOptions.port.displayName": "Port",
			"mssql.connectionOptions.replication.description": "Utilisé par SQL Server dans la réplication",
			"mssql.connectionOptions.replication.displayName": "Réplication",
			"mssql.connectionOptions.serverName.description": "Nom de l'instance SQL Server",
			"mssql.connectionOptions.serverName.displayName": "Serveur",
			"mssql.connectionOptions.trustServerCertificate.description": "Quand la valeur est true (et encrypt=true), SQL Server utilise le chiffrement SSL pour toutes les données envoyées entre le client et le serveur sans valider le certificat de serveur",
			"mssql.connectionOptions.trustServerCertificate.displayName": "Approuver le certificat de serveur",
			"mssql.connectionOptions.typeSystemVersion.description": "Indique le système de type serveur que le fournisseur doit exposer par le biais de DataReader",
			"mssql.connectionOptions.typeSystemVersion.displayName": "Version du système de type",
			"mssql.connectionOptions.userName.description": "Indique l'identifiant utilisateur à utiliser pour la connexion à la source de données",
			"mssql.connectionOptions.userName.displayName": "Nom d'utilisateur",
			"mssql.connectionOptions.workstationId.description": "Nom de la station de travail se connectant à SQL Server",
			"mssql.connectionOptions.workstationId.displayName": "ID de station de travail",
			"mssql.disabled": "Désactivé",
			"mssql.enabled": "Activé",
			"mssql.exportNotebookToSql": "Exporter le notebook au format SQL",
			"mssql.exportSqlAsNotebook": "Exporter SQL au format Notebook",
			"mssql.format.alignColumnDefinitionsInColumns": "Spécifie si les définitions de colonne doivent être alignées",
			"mssql.format.datatypeCasing": "Spécifie si la mise en forme des types de données est MAJUSCULES, minuscules ou aucune (sans mise en forme)",
			"mssql.format.keywordCasing": "Spécifie si la mise en forme des mots clés est MAJUSCULES, minuscules ou aucune (sans mise en forme)",
			"mssql.format.placeCommasBeforeNextStatement": "spécifie si des virgules doivent être placées au début de chaque instruction dans une liste (par exemple : ',mycolumn2') plutôt qu'à la fin (par exemple : 'mycolumn1,')",
			"mssql.format.placeSelectStatementReferencesOnNewLine": "Spécifie si les références aux objets dans les instructions select doivent être divisées en plusieurs lignes. Par exemple, pour 'SELECT C1, C2 FROM T1', C1 et C2 sont sur des lignes distinctes",
			"mssql.ignorePlatformWarning": "[Facultatif] Ne pas afficher les avertissements de plateforme non prise en charge",
			"mssql.intelliSense.enableErrorChecking": "Indique s'il faut activer la vérification des erreurs IntelliSense",
			"mssql.intelliSense.enableIntelliSense": "Indique s'il faut activer IntelliSense",
			"mssql.intelliSense.enableQuickInfo": "Indique s'il faut activer Info express IntelliSense",
			"mssql.intelliSense.enableSuggestions": "Indique s'il faut activer les suggestions IntelliSense",
			"mssql.intelliSense.lowerCaseSuggestions": "Indique si les suggestions IntelliSense doivent être en minuscules",
			"mssql.logDebugInfo": "[Facultatif] Journaliser la sortie de débogage dans la console (Voir -> Sortie) et sélectionner le canal de sortie approprié dans la liste déroulante",
			"mssql.logFilesRemovalLimit": "Nombre maximal d'anciens fichiers ayant dépassé mssql.logRetentionMinutes à supprimer au démarrage. Les fichiers qui ne sont pas nettoyés du fait de cette limitation le sont au prochain démarrage d'Azure Data Studio.",
			"mssql.logRetentionMinutes": "Nombre de minutes de conservation des fichiers journaux pour les services de back-end. La durée par défaut est 1 semaine.",
			"mssql.provider.displayName": "Microsoft SQL Server",
			"mssql.query.alwaysEncryptedParameterization": "Activer la paramétrisation pour Always Encrypted",
			"mssql.query.ansiDefaults": "Activer SET ANSI_DEFAULTS",
			"mssql.query.ansiNullDefaultOn": "Activer SET ANSI_NULL_DFLT_ON",
			"mssql.query.ansiNulls": "Activer SET ANSI_NULLS",
			"mssql.query.ansiPadding": "Activer SET ANSI_PADDING",
			"mssql.query.ansiWarnings": "Activer SET ANSI_WARNINGS",
			"mssql.query.arithAbort": "Activer l'option SET ARITHABORT",
			"mssql.query.cursorCloseOnCommit": "Activer SET CURSOR_CLOSE_ON_COMMIT",
			"mssql.query.deadlockPriority": "Activer l'option SET DEADLOCK_PRIORITY",
			"mssql.query.displayBitAsNumber": "Spécifie si les colonnes BIT doivent être affichées sous forme de nombre (1 ou 0). Si la valeur est false, les colonnes BIT sont affichées sous la forme 'true' ou 'false'",
			"mssql.query.executionTimeout": "Un délai d'expiration de 0 indique une attente illimitée (aucun délai d'expiration)",
			"mssql.query.implicitTransactions": "Activer SET IMPLICIT_TRANSACTIONS",
			"mssql.query.lockTimeout": "Activer l'option SET LOCK TIMEOUT (en millisecondes)",
			"mssql.query.maxXmlCharsToStore": "Nombre de caractères XML à stocker après l'exécution d'une requête",
			"mssql.query.noCount": "Activer l'option SET NOCOUNT",
			"mssql.query.noExec": "Activer l'option SET NOEXEC",
			"mssql.query.parseOnly": "Activer l'option SET PARSEONLY",
			"mssql.query.queryGovernorCostLimit": "Activer SET QUERY_GOVERNOR_COST_LIMIT",
			"mssql.query.quotedIdentifier": "Activer SET QUOTED_IDENTIFIER",
			"mssql.query.setRowCount": "Nombre maximal de lignes à retourner avant que le serveur arrête le traitement de votre requête.",
			"mssql.query.statisticsIO": "Activer l'option SET STATISTICS IO",
			"mssql.query.statisticsTime": "Activer l'option SET STATISTICS TIME",
			"mssql.query.textSize": "Taille maximale des données text et ntext retournées par une instruction SELECT",
			"mssql.query.transactionIsolationLevel": "Activer l'option SET TRANSACTION ISOLATION LEVEL",
			"mssql.query.xactAbortOn": "Activer l'option SET XACT-ABORT ON",
			"mssql.tracingLevel": "[Facultatif] Niveau de journalisation des services de back-end. Azure Data Studio génère un nom de fichier à chaque démarrage et, si le fichier existe déjà, ajoute les entrées de journal à ce fichier. Pour nettoyer les anciens fichiers journaux, consultez les paramètres logRetentionMinutes et logFilesRemovalLimit. Le niveau de suivi par défaut correspond à une faible journalisation. Si vous changez le niveau de détail, vous pouvez obtenir une journalisation massive nécessitant de l'espace disque pour les journaux. Le niveau Erreur inclut le niveau Critique, le niveau Avertissement inclut le niveau Erreur, le niveau Informations inclut le niveau Avertissement et le niveau Détail inclut le niveau Informations",
			"mssqlCluster.copyPath": "Copier le chemin",
			"mssqlCluster.deleteFiles": "Supprimer",
			"mssqlCluster.manageAccess": "Gérer l'accès",
			"mssqlCluster.mkdir": "Nouveau répertoire",
			"mssqlCluster.previewFile": "Aperçu",
			"mssqlCluster.saveFile": "Enregistrer",
			"mssqlCluster.uploadFiles": "Charger des fichiers",
			"notebook.command.new": "Nouveau notebook",
			"notebook.command.open": "Ouvrir le notebook",
			"objectsListProperties.name": "Nom",
			"onprem.databaseProperties.compatibilityLevel": "Niveau de compatibilité",
			"onprem.databaseProperties.lastBackupDate": "Dernière sauvegarde de base de données",
			"onprem.databaseProperties.lastLogBackupDate": "Dernière sauvegarde de journal",
			"onprem.databaseProperties.owner": "Propriétaire",
			"onprem.databaseProperties.recoveryModel": "Mode de récupération",
			"onprem.serverProperties.machineName": "Nom de l'ordinateur",
			"onprem.serverProperties.osVersion": "Version de système d'exploitation",
			"onprem.serverProperties.serverEdition": "Édition",
			"onprem.serverProperties.serverVersion": "Version",
			"tab.bigDataClusterDescription": "Tâches et informations concernant votre cluster Big Data SQL Server",
			"title.bigDataCluster": "Cluster Big Data SQL Server",
			"title.books": "Notebooks",
			"title.clearSearchServerResult": "Recherche : Effacer les résultats du serveur de recherche",
			"title.configurePython": "Configurer Python pour Notebooks",
			"title.designTable": "Conception",
			"title.endpoints": "Points de terminaison de service",
			"title.installPackages": "Installer les packages",
			"title.newSparkJob": "Nouveau travail Spark",
			"title.newTable": "Nouvelle table",
			"title.openClusterDashboard": "Cluster\r\nTableau de bord",
			"title.openSparkHistory": "Voir l'historique Spark",
			"title.openYarnHistory": "Voir l'historique Yarn",
			"title.searchServers": "Recherche : Serveurs",
			"title.showLogFile": "Afficher le fichier journal",
			"title.submitSparkJob": "Envoyer le travail Spark",
			"title.tasks": "Tâches"
		}
	}
}