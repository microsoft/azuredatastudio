{
	"": [
		"--------------------------------------------------------------------------------------------",
		"Copyright (c) Microsoft Corporation. All rights reserved.",
		"Licensed under the Source EULA. See License.txt in the project root for license information.",
		"--------------------------------------------------------------------------------------------",
		"Do not edit this file. It is machine generated."
	],
	"version": "1.0.0",
	"contents": {
		"dist/dashboard/serviceEndpoints": {
			"copyText": "Kopieren",
			"endpoint.appproxy": "Anwendungsproxy",
			"endpoint.controller": "Clusterverwaltungsdienst",
			"endpoint.gateway": "Gateway für den Zugriff auf HDFS-Dateien, Spark",
			"endpoint.grafana": "Metrikdashboard",
			"endpoint.kibana": "Dashboard für Protokollsuche",
			"endpoint.livy": "Proxy zum Ausführen von Spark-Anweisungen, -Aufträgen und -Anwendungen",
			"endpoint.managementproxy": "Verwaltungsproxy",
			"endpoint.mgmtproxy": "Verwaltungsproxy",
			"endpoint.sparkHistory": "Dashboard zum Verwalten und Überwachen von Spark-Aufträgen",
			"endpoint.sqlServerEndpoint": "Front-End der SQL Server-Masterinstanz",
			"endpoint.webhdfs": "HDFS-Dateisystemproxy",
			"endpoint.yarnHistory": "Dashboard zur Spark-Diagnose und -Überwachung",
			"grafana": "Metrikdashboard",
			"kibana": "Dashboard für Protokollsuche",
			"sparkHistory": "Dashboard zum Verwalten und Überwachen von Spark-Aufträgen",
			"yarnHistory": "Dashboard zur Spark-Diagnose und -Überwachung"
		},
		"dist/features": {
			"mssql.canceledLinkedAzureAccountSelection": "Azure Data Studio muss Azure Key Vault kontaktieren, um auf einen Spaltenhauptschlüssel für Always Encrypted zuzugreifen, aber es wurde kein verknüpftes Azure-Konto ausgewählt. Wiederholen Sie die Abfrage, und wählen Sie bei Aufforderung ein verknüpftes Azure-Konto aus.",
			"mssql.chooseLinkedAzureAccount": "Wählen Sie ein verknüpftes Azure-Konto aus:",
			"mssql.insufficientlyPrivelagedAzureAccount": "Das konfigurierte Azure-Konto für \"{0}\" verfügt nicht über ausreichende Berechtigungen für Azure Key Vault, um auf einen Spaltenhauptschlüssel für Always Encrypted zuzugreifen.",
			"mssql.missingLinkedAzureAccount": "Azure Data Studio muss Azure Key Vault kontaktieren, um auf einen Spaltenhauptschlüssel für Always Encrypted zuzugreifen, aber es ist kein verknüpftes Azure-Konto verfügbar. Fügen Sie ein verknüpftes Azure-Konto hinzu, und wiederholen Sie die Abfrage."
		},
		"dist/hdfs/hdfsModel": {
			"mssql.recursivePermissionOpError": "Fehler beim Anwenden von Berechtigungsänderungen: {0}",
			"mssql.recursivePermissionOpProgress": "Die Berechtigungsänderungen werden auf \"{0}\" angewendet.",
			"mssql.recursivePermissionOpStarted": "Die Berechtigungsänderungen werden unter \"{0}\" rekursiv angewendet.",
			"mssql.recursivePermissionOpSucceeded": "Die Berechtigungsänderungen wurden erfolgreich angewendet."
		},
		"dist/hdfs/webhdfs": {
			"webhdfs.httpError400": "Fehlerhafte Anforderung.",
			"webhdfs.httpError401": "Nicht autorisiert",
			"webhdfs.httpError403": "Unzulässig",
			"webhdfs.httpError404": "Nicht gefunden",
			"webhdfs.httpError500": "Interner Serverfehler",
			"webhdfs.invalidDataStructure": "Ungültige Datenstruktur.",
			"webhdfs.missingProperties": "Der WebHDFS-Client kann aufgrund von fehlenden Optionen nicht erstellt werden: ${0}",
			"webhdfs.undefinedArgument": "\"${0}\" ist nicht definiert.",
			"webhdfs.unexpectedRedirect": "Unerwartete Umleitung",
			"webhdfs.unknownError": "Unbekannter Fehler"
		},
		"dist/localizedConstants": {
			"msgMissingNodeContext": "Es wurde ein Knotenbefehl aufgerufen, ohne dass ein Knoten übergeben wurde.",
			"mssql.accessHeader": "Zugriff",
			"mssql.addLabel": "Hinzufügen",
			"mssql.addUserOrGroup": "Benutzer oder Gruppe hinzufügen",
			"mssql.apply": "Anwenden",
			"mssql.applyRecursively": "Rekursiv anwenden",
			"mssql.defaultHeader": "Standard",
			"mssql.defaultUserAndGroups": "Standardbenutzer und -gruppen",
			"mssql.delete": "Löschen",
			"mssql.enterNamePlaceholder": "Namen eingeben",
			"mssql.errorApplyingAclChanges": "Unerwarteter Fehler beim Anwenden von Änderungen: {0}",
			"mssql.everyone": "Beliebige andere Person",
			"mssql.executeHeader": "Ausführen",
			"mssql.group": "Gruppe",
			"mssql.groupLabel": "Gruppe",
			"mssql.inheritDefaultsLabel": "Standardwerte erben",
			"mssql.locationTitle": "Speicherort: ",
			"mssql.manageAccessTitle": "Zugriff verwalten",
			"mssql.namedUsersAndGroups": "Benannte Benutzer und Gruppen",
			"mssql.owner": "Besitzer",
			"mssql.ownerPostfix": "– Besitzer",
			"mssql.owningGroupPostfix": "– Besitzergruppe",
			"mssql.permissionsTitle": "Berechtigungen",
			"mssql.readHeader": "Lesen",
			"mssql.stickyHeader": "Sticky Bit",
			"mssql.userLabel": "Benutzer",
			"mssql.userOrGroupIcon": "Symbol für Benutzer oder Gruppe",
			"mssql.writeHeader": "Schreiben",
			"sparkConnectionRequired": "Stellen Sie eine Verbindung mit dem Spark-Cluster her, bevor Sie den Verlauf von \"{0}\" anzeigen.",
			"sparkJobSubmission.GetApplicationIdFailed": "Fehler beim Abrufen der Anwendungs-ID. {0}",
			"sparkJobSubmission.LocalFileDestinationHint": "Lokale Datei wird in HDFS hochgeladen. ",
			"sparkJobSubmission.LocalFileNotExisted": "Die lokale Datei \"{0}\" ist nicht vorhanden. ",
			"sparkJobSubmission.NoSqlBigDataClusterFound": "Es wurde kein SQL Server-Big Data-Cluster gefunden.",
			"sparkJobSubmission.PrepareSubmitJob": "Der Auftrag \"{0}\" wird übermittelt... ",
			"sparkJobSubmission.PrepareUploadingFile": "Die Datei wird aus dem lokalen Ordner \"{0}\" in den HDFS-Ordner hochgeladen: {1}",
			"sparkJobSubmission.SparkHistoryLinkMessage": "Spark-Verlaufs-URL: {0} ",
			"sparkJobSubmission.SubmissionEndMessage": ".......................... Ende der Spark-Auftragsübermittlung ............................",
			"sparkJobSubmission.SubmitJobFailed": "Fehler bei der Spark-Auftragsübermittlung. {0} ",
			"sparkJobSubmission.SubmitJobFinished": "Der Spark-Auftrag wurde übermittelt.",
			"sparkJobSubmission.UploadingFileFailed": "Fehler beim Hochladen der Datei in den Cluster. {0}",
			"sparkJobSubmission.UploadingFileSucceeded": "Die Datei wurde erfolgreich in den Cluster hochgeladen.",
			"sparkJobSubmission.YarnUIMessage": "YarnUI-URL: {0} "
		},
		"dist/main": {
			"msgSampleCodeDataFrame": "Dieser Beispielcode lädt die Datei in einen Datenrahmen und zeigt die ersten 10 Ergebnisse an.",
			"mssql.errorConvertingToNotebook": "Fehler beim Konvertieren des SQL-Dokuments in ein Notebook: {0}",
			"mssql.errorConvertingToSQL": "Fehler beim Konvertieren des Notebook-Dokuments in SQL: {0}",
			"noController": "Der Controllerendpunkt für diese Instanz wurde nicht gefunden.",
			"notebookFileType": "Notebooks",
			"unsupportedFileType": "Es werden nur IPYNB-Notebooks unterstützt."
		},
		"dist/objectExplorerNodeProvider/cancelableStream": {
			"streamCanceled": "Der Streamvorgang wurde vom Benutzer abgebrochen."
		},
		"dist/objectExplorerNodeProvider/command": {
			"cancel": "Vorgang abbrechen?",
			"cancelTooltip": "Abbrechen",
			"mssql.searchServers": "Servernamen suchen",
			"progress": "$(sync~spin) {0}..."
		},
		"dist/objectExplorerNodeProvider/connection": {
			"connectionInfoOptionsMissingProperties": "In \"connectionInfo.options\" fehlen einige Eigenschaften: {0}",
			"connectionInfoOptionsUndefined": "\"ConnectionInfo.options\" ist nicht definiert.",
			"connectionInfoUndefined": "\"ConnectionInfo\" ist nicht definiert."
		},
		"dist/objectExplorerNodeProvider/fileSources": {
			"maxSizeNotice": "HINWEIS: Diese Datei wurde zur Vorschau bei \"{0}\" abgeschnitten. ",
			"maxSizeReached": "Die Datei wurde zur Vorschau bei \"{0}\" abgeschnitten."
		},
		"dist/objectExplorerNodeProvider/hdfsCommands": {
			"allFiles": "Alle Dateien",
			"copyPathError": "Fehler beim Kopieren des Pfads: {0}",
			"deleteError": "Fehler beim Löschen von Dateien: {0}",
			"enterDirName": "Verzeichnisnamen eingeben",
			"lblUploadFiles": "Hochladen",
			"makingDir": "Das Verzeichnis wird erstellt.",
			"manageAccessError": "Unerwarteter Fehler beim Öffnen des Dialogfelds \"Zugriff verwalten\": {0}",
			"mkDirError": "Fehler beim Erstellen des Verzeichnisses: {0}",
			"mkdirCanceled": "Der Vorgang wurde abgebrochen.",
			"msgDeleteFile": "Möchten Sie diese Datei löschen?",
			"msgDeleteFolder": "Möchten Sie diesen Ordner und den zugehörigen Inhalt löschen?",
			"previewError": "Fehler bei der Vorschau der Datei: {0}",
			"previewing": "Die Vorschau wird generiert.",
			"saveCanceled": "Der Speichervorgang wurde abgebrochen.",
			"saveError": "Fehler beim Speichern der Datei: {0}",
			"saving": "Die HDFS-Dateien werden gespeichert.",
			"uploadCanceled": "Der Uploadvorgang wurde abgebrochen.",
			"uploadError": "Fehler beim Hochladen von Dateien: {0}",
			"uploading": "Dateien werden in HDFS hochgeladen"
		},
		"dist/objectExplorerNodeProvider/hdfsProvider": {
			"errDeleteConnectionNode": "Eine Verbindung kann nicht gelöscht werden. Nur Unterordner und Dateien können gelöscht werden.",
			"errorExpanding": "Fehler: {0}"
		},
		"dist/objectExplorerNodeProvider/objectExplorerNodeProvider": {
			"hdfsFolder": "HDFS",
			"notifyError": "Fehler bei Benachrichtigung über Knotenänderung: {0}",
			"prmptPwd": "Geben Sie das Kennwort für die Verbindung mit HDFS an:",
			"promptUsername": "Geben Sie den Benutzernamen zum Herstellen einer Verbindung mit HDFS an:",
			"rootLabel": "Stamm",
			"sessionNotFound": "Die Sitzung für den Knoten \"{0}\" ist nicht vorhanden."
		},
		"dist/prompts/confirm": {
			"msgNo": "Nein",
			"msgYes": "Ja"
		},
		"dist/sparkFeature/dialog/dialogCommands": {
			"errorNotSqlBigDataCluster": "Der ausgewählte Server gehört nicht zu einem SQL Server-Big Data-Cluster.",
			"selectOtherServer": "Andere SQL Server-Instanz auswählen",
			"sparkJobSubmission.GetFilePathFromSelectedNodeFailed": "Fehler beim Abrufen des Dateipfads: {0}",
			"sparkJobSubmission.NoSqlSelected": "Es ist keine SQL Server-Instanz ausgewählt.",
			"sparkJobSubmission.PleaseSelectSqlWithCluster": "Wählen Sie SQL Server mit Big Data-Cluster aus."
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkAdvancedTab": {
			"sparkJobSubmission.AdvancedTabName": "ERWEITERT",
			"sparkJobSubmission.ReferenceFilesList": "Referenzdateien",
			"sparkJobSubmission.ReferenceFilesListTooltip": "Dateien, die im Executor-Arbeitsverzeichnis platziert werden sollen. Der Dateipfad muss ein HDFS-Pfad sein. Mehrere Pfade müssen durch ein Semikolon (;) voneinander getrennt werden.",
			"sparkJobSubmission.ReferenceJarList": "JAR-Referenzdateien",
			"sparkJobSubmission.ReferenceJarListToolTip": "JAR-Dateien, die im Executor-Arbeitsverzeichnis platziert werden sollen. Der JAR-Pfad muss ein HDFS-Pfad sein. Mehrere Pfade müssen durch ein Semikolon (;) voneinander getrennt werden.",
			"sparkJobSubmission.ReferencePyList": "PY-Referenzdateien",
			"sparkJobSubmission.ReferencePyListTooltip": "PY-Dateien, die im Executor-Arbeitsverzeichnis platziert werden sollen. Der Dateipfad muss ein HDFS-Pfad sein. Mehrere Pfade müssen durch ein Semikolon (;) voneinander getrennt werden.",
			"sparkJobSubmission.configValues": "Konfigurationswerte",
			"sparkJobSubmission.configValuesTooltip": "Liste von Name-Wert-Paaren, die Spark-Konfigurationswerte enthalten. Als JSON-Wörterbuch codiert. Beispiel: '{\"name\":\"wert\", \"name2\":\"wert2\"}'.",
			"sparkJobSubmission.driverCores": "Treiberkerne",
			"sparkJobSubmission.driverCoresTooltip": "Die Anzahl von CPU-Kernen, die dem Treiber zugeordnet werden sollen.",
			"sparkJobSubmission.driverMemory": "Treiberarbeitsspeicher",
			"sparkJobSubmission.driverMemoryTooltip": "Die Menge an Arbeitsspeicher, die dem Treiber zugeordnet werden soll. Geben Sie Einheiten als Teil des Werts an. Beispiel: 512M oder 2G.",
			"sparkJobSubmission.executorCores": "Executorkerne",
			"sparkJobSubmission.executorCoresTooltip": "Die Anzahl von CPU-Kernen, die dem Executor zugeordnet werden sollen.",
			"sparkJobSubmission.executorCount": "Anzahl von Executors",
			"sparkJobSubmission.executorCountTooltip": "Anzahl der auszuführenden Executorinstanzen.",
			"sparkJobSubmission.executorMemory": "Executorspeicher",
			"sparkJobSubmission.executorMemoryTooltip": "Die Menge an Arbeitsspeicher, die dem Executor zugeordnet werden soll. Geben Sie Einheiten als Teil des Werts an. Beispiel: 512M oder 2G.",
			"sparkJobSubmission.queueName": "Warteschlangenname",
			"sparkJobSubmission.queueNameTooltip": "Name der Spark-Warteschlange, in der die Sitzung ausgeführt wird."
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkConfigurationTab": {
			"sparkJobSubmission.Arguments": "Argumente",
			"sparkJobSubmission.ArgumentsTooltip": "Befehlszeilenargumente, die in Ihrer Hauptklasse verwendet werden. Mehrere Argumente müssen durch Leerzeichen voneinander getrennt werden.",
			"sparkJobSubmission.FilePathPlaceHolder": "Pfad zu einer JAR- oder PY-Datei",
			"sparkJobSubmission.GeneralTabName": "ALLGEMEIN",
			"sparkJobSubmission.HDFSFileNotExisted": "Die angegebene HDFS-Datei ist nicht vorhanden. ",
			"sparkJobSubmission.HDFSFileNotExistedWithPath": "\"{0}\" ist nicht im Cluster vorhanden, oder es wurde eine Ausnahme ausgelöst. ",
			"sparkJobSubmission.JobName": "Auftragsname",
			"sparkJobSubmission.JobNamePlaceHolder": "Namen eingeben...",
			"sparkJobSubmission.LocalFileDestinationHintWithPath": "Die ausgewählte lokale Datei wird in HDFS hochgeladen: {0}",
			"sparkJobSubmission.MainClass": "Hauptklasse",
			"sparkJobSubmission.MainFilePath": "JAR-/PY-Datei",
			"sparkJobSubmission.NotSpecifyJARPYPath": "Die JAR-/PY-Eigenschaftsdatei wurde nicht angegeben.",
			"sparkJobSubmission.NotSpecifyJobName": "Der Eigenschaftsauftragsname wurde nicht angegeben.",
			"sparkJobSubmission.NotSpecifyMainClass": "Die Hauptklasse der Eigenschaft wurde nicht angegeben.",
			"sparkJobSubmission.SelectFileError": "Fehler beim Suchen der Datei: {0}",
			"sparkJobSubmission.SparkCluster": "Spark-Cluster",
			"sparkSelectLocalFile": "Auswählen"
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionDialog": {
			"sparkJobSubmission.DialogCancelButton": "Abbrechen",
			"sparkJobSubmission.DialogSubmitButton": "Übermitteln",
			"sparkJobSubmission.DialogTitleNewJob": "Neuer Auftrag",
			"sparkJobSubmission.SparkJobSubmissionDialogInitializeError": "Die Parameter für \"SparkJobSubmissionDialog\" sind ungültig.",
			"sparkJobSubmission.SubmissionStartMessage": ".......................... Start der Spark-Auftragsübermittlung ..........................",
			"sparkJobSubmission.SubmitSparkJob": "{0} Spark-Auftragsübermittlung:"
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionModel": {
			"sparkJobSubmission.GetApplicationIdTimeOut": "Timeout beim Abrufen der Anwendungs-ID. {0}[Protokoll]   {1}",
			"sparkJobSubmission.LivyBatchIdIsInvalid": "\"livyBatchId\" ist ungültig. ",
			"sparkJobSubmission.PathNotSpecified.": "Der Eigenschaftspfad wurde nicht angegeben. ",
			"sparkJobSubmission.SparkJobSubmissionModelInitializeError": "Die Parameter für \"SparkJobSubmissionModel\" sind ungültig.",
			"sparkJobSubmission.localFileOrFolderNotSpecified.": "Die localFilePath- oder hdfsFolderPath-Eigenschaft wurde nicht angegeben. ",
			"sparkJobSubmission.submissionArgsIsInvalid": "\"submissionArgs\" ist ungültig. "
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionService": {
			"sparkJobSubmission.LivyNoBatchIdReturned": "In der Antwort wurde keine Batch-ID für Spark-Aufträge zurückgegeben.{0}[Fehler] {1}",
			"sparkJobSubmission.LivyNoLogReturned": "Innerhalb der Antwort wird kein Protokoll zurückgegeben.{0}[Fehler] {1}"
		},
		"dist/sqlClusterLookUp": {
			"bdcConnectError": "Fehler: {0}. ",
			"promptBDCPassword": "Geben Sie das Kennwort zum Herstellen einer Verbindung mit dem BDC-Controller an.",
			"promptBDCUsername": "{0}Geben Sie den Benutzernamen zum Herstellen einer Verbindung mit dem BDC-Controller an:",
			"usernameAndPasswordRequired": "Benutzername und Kennwort sind erforderlich."
		},
		"dist/sqlToolsServer": {
			"downloadServiceDoneChannelMsg": "Die Installation von {0} wurde abgeschlossen.",
			"downloadingServiceChannelMsg": "\"{0}\" wird heruntergeladen.",
			"downloadingServiceSizeChannelMsg": "({0} KB)",
			"downloadingServiceStatusMsg": "\"{0}\" wird heruntergeladen.",
			"entryExtractedChannelMsg": "{0} extrahiert ({1}/{2})",
			"failedToStartServiceErrorMsg": "Fehler beim Starten von \"{0}\".",
			"installedServiceChannelMsg": "\"{0}\" wurde installiert.",
			"installingServiceChannelMsg": "\"{0}\" wird in \"{1}\" installiert.",
			"installingServiceStatusMsg": "\"{0}\" wird installiert.",
			"serviceStartedStatusMsg": "\"{0}\" wurde gestartet.",
			"startingServiceStatusMsg": "\"{0}\" wird gestartet."
		},
		"dist/telemetry": {
			"serviceCrashMessage": "Die Komponente \"{0}\" wurde unerwartet beendet. Starten Sie Azure Data Studio neu.",
			"viewKnownIssuesText": "Bekannte Probleme anzeigen"
		},
		"package": {
			"cloud.databaseProperties.azureEdition": "Edition",
			"cloud.databaseProperties.compatibilityLevel": "Kompatibilitätsgrad",
			"cloud.databaseProperties.owner": "Besitzer",
			"cloud.databaseProperties.serviceLevelObjective": "Tarif",
			"cloud.serverProperties.serverEdition": "Typ",
			"cloud.serverProperties.serverVersion": "Version",
			"databasesListProperties.lastBackup": "Letzte Sicherung",
			"databasesListProperties.name": "Name",
			"databasesListProperties.size": "Größe (MB)",
			"databasesListProperties.status": "Status",
			"json.format.enable.desc": "Standard-JSON-Formatierer aktivieren/deaktivieren (Neustart erforderlich)",
			"json.schemas.desc": "Hiermit werden Schemas zu JSON-Dateien im aktuellen Projekt zugeordnet.",
			"json.schemas.fileMatch.desc": "Ein Array aus Dateimustern zum Abgleich beim Auflösen von JSON-Dateien in Schemas",
			"json.schemas.fileMatch.item.desc": "Ein Dateimuster, das \"*\" enthalten kann, zum Abgleich beim Auflösen von JSON-Dateien in Schemas",
			"json.schemas.schema.desc": "Die Schemadefinition für die angegebene URL. Das Schema muss nur angegeben werden, um Zugriffe auf die Schema-URL zu vermeiden.",
			"json.schemas.url.desc": "Eine URL zu einem Schema oder ein relativer Pfad zu einem Schema im aktuellen Verzeichnis",
			"mssql.configuration.title": "MSSQL-Konfiguration",
			"mssql.connectionOptions.applicationIntent.description": "Deklariert den Anwendungsauslastungstyp beim Herstellen einer Verbindung mit einem Server.",
			"mssql.connectionOptions.applicationIntent.displayName": "Anwendungszweck",
			"mssql.connectionOptions.applicationName.description": "Der Name der Anwendung",
			"mssql.connectionOptions.applicationName.displayName": "Anwendungsname",
			"mssql.connectionOptions.asynchronousProcessing.description": "Bei Festlegung auf TRUE wird die Verwendung der asynchronen Verarbeitung im .NET Framework-Datenanbieter ermöglicht.",
			"mssql.connectionOptions.asynchronousProcessing.displayName": "Asynchrone Verarbeitung",
			"mssql.connectionOptions.attachDbFilename.displayName": "Dateiname der anzufügenden Datenbank",
			"mssql.connectionOptions.attachedDBFileName.description": "Der Name der primären Datei einer anfügbaren Datenbank, einschließlich des vollständigen Pfadnamens.",
			"mssql.connectionOptions.attachedDBFileName.displayName": "Dateiname der angefügten Datenbank",
			"mssql.connectionOptions.authType.categoryValues.azureMFA": "Azure Active Directory: universell mit MFA-Unterstützung",
			"mssql.connectionOptions.authType.categoryValues.integrated": "Windows-Authentifizierung",
			"mssql.connectionOptions.authType.categoryValues.sqlLogin": "SQL-Anmeldung",
			"mssql.connectionOptions.authType.description": "Gibt die Methode für die Authentifizierung bei SQL Server an.",
			"mssql.connectionOptions.authType.displayName": "Authentifizierungstyp",
			"mssql.connectionOptions.columnEncryptionSetting.description": "Aktiviert oder deaktiviert Always Encrypted für die Verbindung.",
			"mssql.connectionOptions.columnEncryptionSetting.displayName": "Always Encrypted",
			"mssql.connectionOptions.connectRetryCount.description": "Anzahl der Versuche zur Verbindungswiederherstellung",
			"mssql.connectionOptions.connectRetryCount.displayName": "Anzahl der Verbindungswiederholungen",
			"mssql.connectionOptions.connectRetryInterval.description": "Verzögerung zwischen Versuchen zur Verbindungswiederherstellung",
			"mssql.connectionOptions.connectRetryInterval.displayName": "Intervall für Verbindungswiederholung",
			"mssql.connectionOptions.connectTimeout.description": "Die Zeitspanne (in Sekunden), die auf eine Verbindung mit dem Server gewartet wird, bevor der Versuch beendet und ein Fehler generiert wird.",
			"mssql.connectionOptions.connectTimeout.displayName": "Verbindungstimeout",
			"mssql.connectionOptions.connectionName.description": "Benutzerdefinierter Name der Verbindung",
			"mssql.connectionOptions.connectionName.displayName": "Name (optional)",
			"mssql.connectionOptions.contextConnection.description": "Bei Festlegung auf TRUE muss die Verbindung aus dem SQL-Serverkontext stammen. Nur verfügbar bei Ausführung im SQL Server-Prozess.",
			"mssql.connectionOptions.contextConnection.displayName": "Kontextverbindung",
			"mssql.connectionOptions.currentLanguage.description": "Der Datensatzname der SQL Server-Sprache",
			"mssql.connectionOptions.currentLanguage.displayName": "Aktuelle Sprache",
			"mssql.connectionOptions.databaseName.description": "Der Name des anfänglichen Katalogs oder der ersten Datenbank in der Datenquelle",
			"mssql.connectionOptions.databaseName.displayName": "Datenbank",
			"mssql.connectionOptions.enclaveAttestationProtocol.categoryValues.AAS": "Azure Attestation",
			"mssql.connectionOptions.enclaveAttestationProtocol.categoryValues.HGS": "Host-Überwachungsdienst",
			"mssql.connectionOptions.enclaveAttestationProtocol.description": "Gibt ein Protokoll zum Nachweis einer serverseitigen Enclave an, die mit Always Encrypted für Secure Enclaves verwendet wird.",
			"mssql.connectionOptions.enclaveAttestationProtocol.displayName": "Nachweisprotokoll",
			"mssql.connectionOptions.enclaveAttestationUrl.description": "Gibt einen Endpunkt zum Nachweis einer serverseitigen Enclave an, die mit Always Encrypted für Secure Enclaves verwendet wird.",
			"mssql.connectionOptions.enclaveAttestationUrl.displayName": "Enclave-Nachweis-URL",
			"mssql.connectionOptions.encrypt.description": "Bei Festlegung auf TRUE verwendet SQL Server die SSL-Verschlüsselung für alle zwischen Client und Server gesendeten Daten, sofern auf dem Server ein Zertifikat installiert ist.",
			"mssql.connectionOptions.encrypt.displayName": "Verschlüsseln",
			"mssql.connectionOptions.failoverPartner.description": "Der Name oder die Netzwerkadresse der SQL Server-Instanz, die als Failoverpartner fungiert",
			"mssql.connectionOptions.failoverPartner.displayName": "Failoverpartner",
			"mssql.connectionOptions.groupName.advanced": "Erweitert",
			"mssql.connectionOptions.groupName.connectionResiliency": "Verbindungsresilienz",
			"mssql.connectionOptions.groupName.context": "Kontext",
			"mssql.connectionOptions.groupName.initialization": "Initialisierung",
			"mssql.connectionOptions.groupName.pooling": "Pooling",
			"mssql.connectionOptions.groupName.replication": "Replikation",
			"mssql.connectionOptions.groupName.security": "Sicherheit",
			"mssql.connectionOptions.groupName.source": "Quelle",
			"mssql.connectionOptions.loadBalanceTimeout.description": "Die Mindestzeitspanne (in Sekunden), für die diese Verbindung im Pool verbleiben soll, bevor sie zerstört wird",
			"mssql.connectionOptions.loadBalanceTimeout.displayName": "Timeout für Lastenausgleich",
			"mssql.connectionOptions.maxPoolSize.description": "Die maximal zulässige Anzahl von Verbindungen im Pool",
			"mssql.connectionOptions.maxPoolSize.displayName": "Maximale Poolgröße",
			"mssql.connectionOptions.minPoolSize.description": "Die mindestens erforderliche Anzahl von Verbindungen im Pool",
			"mssql.connectionOptions.minPoolSize.displayName": "Minimale Poolgröße",
			"mssql.connectionOptions.multiSubnetFailover.displayName": "Multisubnetzfailover",
			"mssql.connectionOptions.multipleActiveResultSets.description": "Bei Festlegung auf TRUE können mehrere Resultsets zurückgegeben und aus einer Verbindung gelesen werden.",
			"mssql.connectionOptions.multipleActiveResultSets.displayName": "Mehrere aktive Resultsets",
			"mssql.connectionOptions.packetSize.description": "Größe der Netzwerkpakete (in Byte), die bei der Kommunikation mit einer Instanz von SQL Server verwendet werden",
			"mssql.connectionOptions.packetSize.displayName": "Paketgröße",
			"mssql.connectionOptions.password.description": "Gibt das Kennwort an, das beim Herstellen einer Verbindung mit der Datenquelle verwendet werden soll.",
			"mssql.connectionOptions.password.displayName": "Kennwort",
			"mssql.connectionOptions.persistSecurityInfo.description": "Bei Festlegung auf FALSE werden sicherheitsrelevante Informationen, z. B. das Kennwort, nicht als Teil der Verbindung zurückgegeben.",
			"mssql.connectionOptions.persistSecurityInfo.displayName": "Sicherheitsinformationen dauerhaft speichern",
			"mssql.connectionOptions.pooling.description": "Bei Festlegung auf TRUE wird das Verbindungsobjekt aus dem geeigneten Pool abgerufen oder bei Bedarf erstellt und dem geeigneten Pool hinzugefügt.",
			"mssql.connectionOptions.pooling.displayName": "Pooling",
			"mssql.connectionOptions.port.displayName": "Port",
			"mssql.connectionOptions.replication.description": "Wird von SQL Server bei der Replikation verwendet.",
			"mssql.connectionOptions.replication.displayName": "Replikation",
			"mssql.connectionOptions.serverName.description": "Name der SQL Server-Instanz",
			"mssql.connectionOptions.serverName.displayName": "Server",
			"mssql.connectionOptions.trustServerCertificate.description": "Bei Festlegung auf TRUE (und encrypt=true) verwendet SQL Server die SSL-Verschlüsselung für alle zwischen Client und Server gesendeten Daten, ohne das Serverzertifikat zu überprüfen.",
			"mssql.connectionOptions.trustServerCertificate.displayName": "Serverzertifikat vertrauen",
			"mssql.connectionOptions.typeSystemVersion.description": "Gibt an, welches Servertypsystem der Anbieter über den DataReader offenlegt.",
			"mssql.connectionOptions.typeSystemVersion.displayName": "Typsystemversion",
			"mssql.connectionOptions.userName.description": "Gibt die Benutzer-ID an, die beim Herstellen einer Verbindung mit der Datenquelle verwendet werden soll.",
			"mssql.connectionOptions.userName.displayName": "Benutzername",
			"mssql.connectionOptions.workstationId.description": "Der Name der Arbeitsstation, die eine Verbindung mit SQL Server herstellt",
			"mssql.connectionOptions.workstationId.displayName": "Arbeitsstations-ID",
			"mssql.disabled": "Deaktiviert",
			"mssql.enabled": "Aktiviert",
			"mssql.exportNotebookToSql": "Notebook als SQL exportieren",
			"mssql.exportSqlAsNotebook": "SQL als Notebook exportieren",
			"mssql.format.alignColumnDefinitionsInColumns": "Sollen Spaltendefinitionen ausgerichtet werden?",
			"mssql.format.datatypeCasing": "Gibt an, ob Datentypen in Großbuchstaben, Kleinbuchstaben oder gar nicht formatiert werden sollen.",
			"mssql.format.keywordCasing": "Gibt an, ob Schlüsselwörter in Großbuchstaben, Kleinbuchstaben oder gar nicht formatiert werden sollen.",
			"mssql.format.placeCommasBeforeNextStatement": "Gibt an, dass Kommas in einer Liste am Anfang der einzelnen Anweisungen (z. B. \", mycolumn2\") und nicht am Ende platziert werden sollen: \"mycolumn1,\"",
			"mssql.format.placeSelectStatementReferencesOnNewLine": "Sollen Verweise auf Objekte in einer SELECT-Anweisung in separaten Zeilen angezeigt werden? Beispielsweise werden bei \"SELECT C1, C2 FROM T1\" C1 und C2 jeweils in separaten Zeilen angezeigt.",
			"mssql.ignorePlatformWarning": "[Optional] Keine Anzeige von Warnungen zu nicht unterstützten Plattformen.",
			"mssql.intelliSense.enableErrorChecking": "Gibt an, ob die IntelliSense-Fehlerüberprüfung aktiviert werden soll.",
			"mssql.intelliSense.enableIntelliSense": "Gibt an, ob IntelliSense aktiviert werden soll.",
			"mssql.intelliSense.enableQuickInfo": "Gibt an, ob IntelliSense-QuickInfo aktiviert werden soll.",
			"mssql.intelliSense.enableSuggestions": "Gibt an, ob IntelliSense-Vorschläge aktiviert werden sollen.",
			"mssql.intelliSense.lowerCaseSuggestions": "Gibt an, ob IntelliSense-Vorschläge in Kleinbuchstaben angezeigt werden sollen.",
			"mssql.logDebugInfo": "[Optional] Protokollieren Sie die Debugausgabe in der Konsole (Ansicht > Ausgabe), und wählen Sie dann in der Dropdownliste den geeigneten Ausgabekanal aus.",
			"mssql.logFilesRemovalLimit": "Die maximale Anzahl alter Dateien, die beim Start entfernt werden sollen, bei denen der mssql.logRetentionMinutes-Wert abgelaufen ist. Dateien, die aufgrund dieser Einschränkung nicht bereinigt werden, werden beim nächsten Start von Azure Data Studio bereinigt.",
			"mssql.logRetentionMinutes": "Anzahl von Minuten, für die Protokolldateien für Back-End-Dienste aufbewahrt werden sollen. Der Standardwert ist 1 Woche.",
			"mssql.provider.displayName": "Microsoft SQL Server",
			"mssql.query.alwaysEncryptedParameterization": "Parametrisierung für Always Encrypted aktivieren",
			"mssql.query.ansiDefaults": "SET ANSI_DEFAULTS aktivieren",
			"mssql.query.ansiNullDefaultOn": "SET ANSI_NULL_DFLT_ON aktivieren",
			"mssql.query.ansiNulls": "SET ANSI_NULLS aktivieren",
			"mssql.query.ansiPadding": "SET ANSI_PADDING aktivieren",
			"mssql.query.ansiWarnings": "SET ANSI_WARNINGS aktivieren",
			"mssql.query.arithAbort": "Set ARITHABORT-Option aktivieren",
			"mssql.query.cursorCloseOnCommit": "SET CURSOR_CLOSE_ON_COMMIT aktivieren",
			"mssql.query.deadlockPriority": "SET DEADLOCK_PRIORITY-Option aktivieren",
			"mssql.query.displayBitAsNumber": "BIT-Spalten als Zahlen (1 oder 0) anzeigen? Bei Festlegung auf FALSE werden BIT-Spalten als TRUE oder FALSE angezeigt.",
			"mssql.query.executionTimeout": "Ein Timeoutwert von 0 für die Ausführung kennzeichnet einen unbegrenzten Wartevorgang (kein Timeout).",
			"mssql.query.implicitTransactions": "SET IMPLICIT_TRANSACTIONS aktivieren",
			"mssql.query.lockTimeout": "SET LOCK TIMEOUT-Option aktivieren (in Millisekunden)",
			"mssql.query.maxXmlCharsToStore": "Anzahl von XML-Zeichen, die nach dem Ausführen einer Abfrage gespeichert werden sollen",
			"mssql.query.noCount": "SET NOCOUNT-Option aktivieren",
			"mssql.query.noExec": "SET NOEXEC-Option aktivieren",
			"mssql.query.parseOnly": "SET PARSEONLY-Option aktivieren",
			"mssql.query.queryGovernorCostLimit": "SET QUERY_GOVERNOR_COST_LIMIT aktivieren",
			"mssql.query.quotedIdentifier": "SET QUOTED_IDENTIFIER aktivieren",
			"mssql.query.setRowCount": "Maximale Anzahl von Zeilen, die zurückgegeben werden sollen, bevor der Server die Verarbeitung Ihrer Abfrage beendet.",
			"mssql.query.statisticsIO": "Set STATISTICS IO-Option aktivieren",
			"mssql.query.statisticsTime": "SET STATISTICS TIME-Option aktivieren",
			"mssql.query.textSize": "Maximale Größe von text- und ntext-Daten, die von einer SELECT-Anweisung zurückgegeben werden",
			"mssql.query.transactionIsolationLevel": "SET TRANSACTION ISOLATION LEVEL-Option aktivieren",
			"mssql.query.xactAbortOn": "SET XACT_ABORT ON-Option aktivieren",
			"mssql.tracingLevel": "[Optional] Protokolliergrad für Back-End-Dienste. Azure Data Studio generiert bei jedem Start einen Dateinamen, und falls die Datei bereits vorhanden ist, werden die Protokolleinträge an diese Datei angehängt. Zur Bereinigung alter Protokolldateien können die Einstellungen \"logRetentionMinutes\" und \"logFilesRemovalLimit\" verwendet werden. Bei Verwendung des Standardwerts für \"tracingLevel\" werden nur wenige Informationen protokolliert. Eine Änderung der Ausführlichkeit kann zu einem umfangreichen Protokollierungsaufkommen und einem hohen Speicherplatzbedarf für die Protokolle führen. \"Error\" umfasst kritische Meldungen, \"Warning\" umfasst alle Daten aus \"Error\" sowie Warnmeldungen, \"Information\" umfasst alle Daten aus \"Warning\" sowie Informationsmeldungen, \"Verbose\" umfasst ausführliche Informationen.",
			"mssqlCluster.copyPath": "Pfad kopieren",
			"mssqlCluster.deleteFiles": "Löschen",
			"mssqlCluster.manageAccess": "Zugriff verwalten",
			"mssqlCluster.mkdir": "Neues Verzeichnis",
			"mssqlCluster.previewFile": "Vorschau",
			"mssqlCluster.saveFile": "Speichern",
			"mssqlCluster.uploadFiles": "Dateien hochladen",
			"notebook.command.new": "Neues Notebook",
			"notebook.command.open": "Notebook öffnen",
			"objectsListProperties.name": "Name",
			"onprem.databaseProperties.compatibilityLevel": "Kompatibilitätsgrad",
			"onprem.databaseProperties.lastBackupDate": "Letzte Datenbanksicherung",
			"onprem.databaseProperties.lastLogBackupDate": "Letzte Protokollsicherung",
			"onprem.databaseProperties.owner": "Besitzer",
			"onprem.databaseProperties.recoveryModel": "Wiederherstellungsmodell",
			"onprem.serverProperties.machineName": "Computername",
			"onprem.serverProperties.osVersion": "Betriebssystemversion",
			"onprem.serverProperties.serverEdition": "Edition",
			"onprem.serverProperties.serverVersion": "Version",
			"tab.bigDataClusterDescription": "Aufgaben und Informationen zu Ihrem SQL Server-Big Data-Cluster",
			"title.bigDataCluster": "SQL Server-Big Data-Cluster",
			"title.books": "Notebooks",
			"title.clearSearchServerResult": "Suche: Suchserverergebnisse löschen",
			"title.configurePython": "Python für Notebooks konfigurieren",
			"title.endpoints": "Dienstendpunkte",
			"title.installPackages": "Pakete installieren",
			"title.newSparkJob": "Neuer Spark-Auftrag",
			"title.openClusterDashboard": "Cluster\r\nDashboard",
			"title.openSparkHistory": "Spark-Verlauf anzeigen",
			"title.openYarnHistory": "Yarn-Verlauf anzeigen",
			"title.searchServers": "Suche: Server",
			"title.showLogFile": "Protokolldatei anzeigen",
			"title.submitSparkJob": "Spark-Auftrag übermitteln",
			"title.tasks": "Aufgaben"
		}
	}
}